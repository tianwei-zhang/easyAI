#' Deep Learning Classification with Automated Parameter Tuning
#' @param x training feature matrix
#' @param y target matrix
#' @param num_layer a vector of integers indicating the number of hidden layers to test. Default to seq(1,5,1)
#' @param max_units the maximum number of hidden units in a layer. Default to an optimized value based on data
#' @param start_unit the minimum number of hiddent units in a layer. Default to 5
#' @param max_dropout A number between 0 and 1 indicating the maximum dropoff rate in a layer. Default to 0.2
#' @param min_dropout A number between 0 and 1 indicating the minimum dropoff rate in a layer. Default to 0
#' @param max_lr maximum learning rate in a run. Default to 0.2
#' @param min_lr minimum learning rate in a run. Default to 0.001
#' @param iteration_per_layer Number of parameter randomizations for a given number of hidden layers. More iterations will explore a larger parameter space
#' @param num_epoch number of epoches to go through during training
#' @param num_patience number of patience in early stopping criteria
#' @return returns a list object with two values:
#' train_performance: A table with parameters and model performance metrics
#' best_model: a keras_model object with the optimal parameters
#' @export

deep_logistic=function(x,
                       y,
                       # optimizer parameters
                       num_layer=seq(1,5,1),
                       max_units=NULL,
                       start_unit=5,
                       max_dropout=0.2,
                       min_dropout=0,
                       max_lr=0.2,
                       min_lr=0.001,
                       iteration_per_layer=5,

                       # model parameters
                       num_epoch=5,
                       num_patience=3
){
  set.seed(0)
  if(is.null(max_units)){
    max_units=round(nrow(x)/(2*(ncol(x)+ncol(y))))
  }
  ## Create training and test set
  if(nrow(x)!=nrow(y)){
    stop('Length of input and target is not the same')
  }
  dir=getwd()
  n=nrow(x)
  train_size=round(0.8*n)
  train_index=sample(train_size,1:n)
  x_train=x[train_index,]
  x_test=x[-train_index,]

  y_train=y[train_index,]
  y_test=y[-train_index,]

  train_file='
  library(keras)
  #Set Flag
  FLAGS <- flags(
  flag_integer("layer1", 10),
  flag_numeric("dropout1",0.1)
  )

  # structure model
  model=keras_model_sequential()
  model=model%>%
    layer_dense(batch_input_shape =list(NULL,ncol(x)) ,
                units = ncol(x),
                activation = "relu",
                kernel_initializer = "normal")%>%
    layer_dense(units = FLAGS$layer1,activation = "relu")%>%
    layer_dropout(rate =FLAGS$dropout1)%>%
    layer_dense(units=ncol(y),activation = "softmax")

    # compile model
    model%>%
      compile(
        optimizer = optimizer_adam(lr=ls),
        loss = "categorical_crossentropy",
        metrics = c("accuracy")
      )

      # Fit models
      early_stopping <- callback_early_stopping(monitor = "val_loss", patience = num_patience)
      model%>%
        fit(
          x=x_train,
          y=y_train,
          validation_data=list[x_test,y_test]
          callbacks = c(early_stopping),epochs = num_epoch,verbose = 0
        )
    '
    cat(paste0('Saving train.R to',dir,'/train.R'))
    write(train_file,file.path(paste0(dir,'/train.R')))

    config_file=paste0('
    trainingInput:
      scaleTier: CUSTOM
      masterType: standard_gpu
      hyperparameters:
        goal: MINIMIZE
        hyperparameterMetricTag: val_loss
        maxTrials: 100
        maxParallelTrials: 100
        params:
          - parameterName: layer1
            type: INTEGER
            minValue: ',start_unit,'
            maxValue: ',max_units,'
            scaleType: UNIT_LINEAR_SCALE
          - parameterName: dropout1
            type: DOUBLE
            minValue: ',min_dropout,'
            maxValue: ',max_lr,'
            scaleType: UNIT_LOG_SCALE
    ')
    cat(paste0('Saving config.yml to',dir,'/config.yml'))
    write(config_file,file.path(paste0(dir,'/confi.yml')))
    cloudml_train(file = 'train.R',config ='tuning.yml')

}
